# ğŸ§¹ Data-Cleaning-Tool-for-SLM-Models  
### *A High-Performance Text Preprocessing Pipeline for Small Language Models*

The **Data Cleaning Pipeline for SLM Models** is a powerful Python-based system designed to transform raw, noisy text data into **clean, structured, and model-ready datasets**. Built with a focus on **NLP and AI workflows**, this pipeline ensures that your data is optimized for training **Small Language Models (SLMs)**, chatbots, and other intelligent systems.

Raw text from sources like web scraping often contains inconsistencies such as HTML tags, special characters, and irrelevant noise. This project eliminates those issues and delivers **high-quality, normalized text** ready for machine learning.

---

<p align="center">
  <strong>âš¡ CleanText Pipeline</strong><br/>
  <em>From Raw Noise â†’ Intelligent Data</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.x-blue?style=flat-square&logo=python"/>
  <img src="https://img.shields.io/badge/NLP-Preprocessing-green?style=flat-square"/>
  <img src="https://img.shields.io/badge/Regex-Cleaning-orange?style=flat-square"/>
  <img src="https://img.shields.io/badge/Lightweight-Fast-lightgrey?style=flat-square"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square"/>
</p>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Objectives](#-objectives)
- [Key Features](#-key-features)
- [Technology Stack](#-technology-stack)
- [Pipeline Architecture](#-pipeline-architecture)
- [Core Processing Steps](#-core-processing-steps)
- [Workflow](#-workflow)
- [Project Structure](#-project-structure)
- [How to Run](#-how-to-run)
- [Use Cases](#-use-cases)
- [Future Enhancements](#-future-enhancements)

---

## ğŸŒŸ Overview

This project focuses on building a **robust and reusable text preprocessing pipeline** tailored for AI and NLP applications.

It solves a critical problem in machine learning:

> â— *â€œGarbage in â†’ Garbage outâ€*

By cleaning and normalizing raw datasets, this system ensures:

- High-quality training data  
- Reduced noise and redundancy  
- Improved model accuracy  
- Consistent and structured input  

---

## ğŸ¯ Objectives

- ğŸ§¹ Remove unwanted noise and symbols  
- ğŸ”¤ Normalize and standardize text  
- ğŸ§  Improve dataset quality for NLP models  
- ğŸ” Build a reusable preprocessing pipeline  
- âš¡ Optimize data for SLM training  

---

## âœ¨ Key Features

| Feature | Description |
|--------|------------|
| ğŸ§¹ **Advanced Cleaning** | Removes HTML tags, URLs, emails, and unwanted characters |
| ğŸ”¤ **Text Normalization** | Converts text to lowercase and standardizes encoding |
| ğŸ§  **Smart Filtering** | Eliminates short, repetitive, and low-quality text |
| âš¡ **Efficient Processing** | Lightweight and fast pipeline using Python |
| ğŸ¤– **Model-Ready Output** | Clean text suitable for NLP, chatbots, and SLM training |

---

## ğŸ›  Technology Stack

| Component | Technology | Purpose |
|----------|-----------|--------|
| **Language** | Python 3.x | Core processing logic |
| **Regex Engine** | `re` | Pattern-based text cleaning |
| **Unicode Handling** | `unicodedata` | Text normalization |
| **Utilities** | `string` | Character filtering |
| **Optional NLP** | `nltk` | Tokenization & advanced preprocessing |

---

## ğŸ— Pipeline Architecture

```
Raw Data â†’ Cleaning â†’ Normalization â†’ Filtering â†’ Clean Dataset
```

### Processing Layers

1. **Input Layer** â†’ Raw text ingestion  
2. **Cleaning Layer** â†’ Noise & unwanted data removal  
3. **Normalization Layer** â†’ Standardization of text  
4. **Filtering Layer** â†’ Quality-based data filtering  
5. **Output Layer** â†’ Clean dataset ready for ML  

---

## ğŸ“œ Core Processing Steps

### ğŸ§¹ Remove HTML Tags
- Strips `<tags>` and markup content  

### ğŸŒ Remove URLs & Emails
- Eliminates hyperlinks and email patterns  

### ğŸ”¤ Normalize Text
- Unicode â†’ ASCII conversion  
- Lowercase transformation  

### âš™ Regex-Based Cleaning
- Removes special characters and symbols  

### ğŸ§  Smart Filtering
- Removes:
  - Very short sentences  
  - Repetitive/noisy text  
  - Low-quality content  

---

## ğŸ”„ Workflow

```
1. Load raw text dataset
2. Remove HTML tags
3. Remove URLs and emails
4. Normalize unicode characters
5. Clean special characters
6. Convert text to lowercase
7. Remove extra whitespace
8. Filter low-quality text
9. Save cleaned dataset
```

---

## ğŸ“ Project Structure

```
Data-Cleaning-SLM/
â”‚
â”œâ”€â”€ cleaning_script.py        # Core preprocessing pipeline
â”œâ”€â”€ input_data.txt            # Raw input dataset
â”œâ”€â”€ cleaned_output.txt        # Final cleaned output
```

---

## âš¡ How to Run

### Prerequisites
- Python 3.x installed  

---

### 1ï¸âƒ£ Install Dependencies (Optional)
```bash
pip install nltk
```

### 2ï¸âƒ£ Run the Script
```bash
python cleaning_script.py
```

### 3ï¸âƒ£ Output File
```
cleaned_output.txt
```

---

## ğŸ¯ Use Cases

- ğŸ§  Training Small Language Models (SLMs)  
- ğŸ¤– Chatbot dataset preparation  
- ğŸ“„ NLP preprocessing pipelines  
- ğŸ” Research data cleaning  
- ğŸŒ Web scraped data processing  

---

## ğŸŒŸ Highlights

âœ” Lightweight and fast pipeline  
âœ” Designed for real-world noisy datasets  
âœ” Easy to extend and customize  
âœ” Focused on AI/NLP workflows  
âœ” Clean, modular implementation  

---

## ğŸ”® Future Enhancements

- ğŸ§  Stopword removal integration  
- ğŸ“Š Tokenization & lemmatization  
- ğŸŒ Integration with web scraping tools  
- âš¡ Batch processing for large datasets  
- ğŸ¤– Direct ML pipeline integration  

---

## ğŸ‘¨â€ğŸ’» Author

**Vaibhav Sharma**  
*AI Developer | Data Pipeline Engineer*

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¡ Final Note

> **High-quality data is the foundation of powerful AI systems.**

This project ensures your dataset is **clean, consistent, and optimized** for training intelligent models ğŸš€

---

<p align="center">
  Built with â¤ï¸ using Python & NLP techniques<br/>
  <strong>CleanText Pipeline</strong> â€” Powering Smarter AI Models
</p>
