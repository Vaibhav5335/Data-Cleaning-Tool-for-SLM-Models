# Data-Cleaning-Tool-for-SLM-Models

# ğŸ§¹ Data Cleaning Pipeline for Training SLM (Small Language Model)

### Advanced Text Preprocessing & Noise Removal System in Python

---

## ğŸ“Œ Overview

This project focuses on building a **robust text cleaning and preprocessing pipeline** designed specifically for preparing high-quality datasets for training **Small Language Models (SLMs)**.

Raw text collected from sources like web scraping often contains:

* âŒ Noise
* âŒ Special characters
* âŒ HTML tags
* âŒ Irrelevant symbols

This system transforms such raw data into **clean, structured, and model-ready text**.

---

## ğŸ¯ Objectives

* Remove unwanted characters and noise from text
* Normalize and standardize textual data
* Improve dataset quality for ML/NLP training
* Build a reusable preprocessing pipeline

---

## ğŸš€ Key Features

### ğŸ§¹ Advanced Text Cleaning

* Removes:

  * Special characters
  * HTML tags
  * URLs
  * Emails
* Filters unwanted symbols using regex

---

### ğŸ”¤ Text Normalization

* Converts text to lowercase
* Removes extra whitespace
* Standardizes encoding (ASCII normalization)

---

### ğŸ§  Smart Filtering

* Removes:

  * Very short sentences
  * Repetitive/noisy text
  * Low-quality data

---

### âš¡ Model-Ready Output

* Produces clean text suitable for:

  * NLP models
  * SLM training
  * Chatbots

---

## ğŸ—ï¸ Project Structure

```id="cleanstruct"
Data-Cleaning-SLM/
â”‚
â”œâ”€â”€ cleaning_script.py        # Core cleaning logic
â”œâ”€â”€ input_data.txt            # Raw dataset
â”œâ”€â”€ cleaned_output.txt        # Processed dataset
```

---

## ğŸ–¥ï¸ Tech Stack

### ğŸ Language

* Python

### ğŸ“¦ Libraries Used

* `re` â†’ Regular expressions for cleaning
* `unicodedata` â†’ Text normalization
* `string` â†’ Character handling
* (Optional) `nltk` â†’ Tokenization

---

## ğŸ”„ Working Pipeline

```id="cleanflow"
1. Load raw text data
2. Remove HTML tags
3. Remove URLs and emails
4. Normalize unicode characters
5. Remove special symbols
6. Convert to lowercase
7. Remove extra spaces
8. Filter low-quality text
9. Save cleaned output
```

---

## ğŸ“œ Core Logic Explained

### ğŸ“Œ Step 1: Remove HTML Tags

* Strips `<tags>` from raw text

---

### ğŸ“Œ Step 2: Remove URLs & Emails

* Cleans hyperlinks and email patterns

---

### ğŸ“Œ Step 3: Normalize Text

* Converts Unicode â†’ ASCII
* Ensures consistent encoding

---

### ğŸ“Œ Step 4: Regex Cleaning

* Removes unwanted characters using patterns

---

### ğŸ“Œ Step 5: Filtering

* Keeps only meaningful sentences
* Removes noise and junk data

---

## âš¡ How to Run

### 1ï¸âƒ£ Install Dependencies (if any)

```bash id="instclean"
pip install nltk
```

---

### 2ï¸âƒ£ Run Script

```bash id="runclean"
python cleaning_script.py
```

---

### 3ï¸âƒ£ Output

* Cleaned text will be saved in:

```id="outclean"
cleaned_output.txt
```

---

## ğŸ“Š Use Cases

* ğŸ§  Training Small Language Models (SLM)
* ğŸ¤– Chatbot dataset preparation
* ğŸ“„ NLP preprocessing pipelines
* ğŸ” Data cleaning for research
* ğŸŒ Web scraped data processing

---

## ğŸŒŸ Highlights

âœ” Clean and reusable pipeline
âœ” Focused on NLP/AI use cases
âœ” Handles real-world noisy data
âœ” Easy to extend and customize
âœ” Lightweight and efficient

---

## ğŸ§© Future Enhancements

* ğŸ§  Add stopword removal
* ğŸ“Š Add tokenization & lemmatization
* ğŸŒ Integrate with web crawlers
* âš¡ Batch processing for large datasets
* ğŸ¤– Direct pipeline for model training

---

## ğŸ‘¨â€ğŸ’» Author

**Vaibhav Sharma**

* AI & Data Enthusiast
* Focused on building intelligent data pipelines

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¡ Final Note

High-quality data is the backbone of any AI model.
This project ensures your dataset is **clean, consistent, and ready for training powerful language models ğŸš€**

---
